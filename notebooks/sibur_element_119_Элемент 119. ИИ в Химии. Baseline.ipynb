{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Создание QSAR модели для предсказания коэффициента липофильности LogP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy pandas scikit-learn chython ipykernel openbabel-wheel pytorch-lightning rdkit torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "from rdkit.Chem import Descriptors, MolFromSmiles, rdFingerprintGenerator as fp\n",
    "\n",
    "from chython import smiles\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pyl\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 1. Разведочный анализ и подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train_data.csv')\n",
    "test_data = pd.read_csv('test_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "Создадим валидационную выборку (отберем случайным образом из обучающей): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1318\n",
    "random_indices = np.random.choice(train_data.index, size=n, replace=False)\n",
    "\n",
    "val_data = train_data.loc[random_indices]\n",
    "train_data_remain = train_data.drop(random_indices)\n",
    "\n",
    "print(f\"Исходный DataFrame: {len(train_data)} строк\")\n",
    "print(f\"Удалено строк: {len(val_data)}\")\n",
    "print(f\"Осталось строк: {len(train_data_remain)}\")\n",
    "train_data = train_data_remain\n",
    "print(val_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_smiles_list = test_data['SMILES'].tolist()\n",
    "train_smiles_list = train_data['SMILES'].tolist()\n",
    "val_smiles_list = val_data['SMILES'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mols = [smiles(m) for m in train_smiles_list]\n",
    "test_mols = [smiles(m) for m in test_smiles_list]\n",
    "val_mols = [smiles(m) for m in val_smiles_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_mols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "#### Проведем стандартизацию химических структур:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(mol_list):\n",
    "    for m in mol_list:\n",
    "        try:\n",
    "            m.clean_stereo()\n",
    "            m.canonicalize()\n",
    "        except:\n",
    "            print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "standardize(train_mols)\n",
    "standardize(test_mols)\n",
    "standardize(val_mols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "#### Преобразуем молекулы в объекты библиотеки RDKit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rdkit_mols = [MolFromSmiles(str(m)) for m in train_mols]\n",
    "test_rdkit_mols = [MolFromSmiles(str(m)) for m in test_mols]\n",
    "val_rdkit_mols = [MolFromSmiles(str(m)) for m in val_mols]\n",
    "val_rdkit_mols[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "#### 1.1 Генерация дескрипторов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_fingerprints(mols):\n",
    "    \"\"\" генерация молекулярных отпечатков по методу Моргана с радиусом 3 и длиной 2048\n",
    "    \"\"\"\n",
    "    morgan_fpgenerator = fp.GetMorganGenerator(radius=3, fpSize=2048)\n",
    "    return pd.DataFrame([morgan_fpgenerator.GetFingerprintAsNumPy(m) for m in mols])\n",
    "\n",
    "PhisChemDescriptors = {\"MR\": Descriptors.MolMR,\n",
    "                       \"TPSA\": Descriptors.TPSA}\n",
    "\n",
    "# функция для генерации дескрипторов из молекул\n",
    "def mol_dsc_calc(mols): \n",
    "    return pd.DataFrame({k: f(m) for k, f in PhisChemDescriptors.items()} \n",
    "                     for m in mols)\n",
    "\n",
    "descriptors_names = PhisChemDescriptors.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "Сгенерируем дескрипторы и молекулярные отпечатки и сохраним их в отдельные датафреймы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptors_datasets = defaultdict(dict)\n",
    "\n",
    "descriptors_transformer = FunctionTransformer(mol_dsc_calc, validate=False)\n",
    "morgan_transformer = FunctionTransformer(calc_fingerprints, validate=False)\n",
    "\n",
    "train_rdkit_data = pd.DataFrame(train_rdkit_mols, columns=['molecules'])\n",
    "test_rdkit_data = pd.DataFrame(test_rdkit_mols, columns=['molecules'])\n",
    "val_rdkit_data = pd.DataFrame(val_rdkit_mols, columns=['molecules'])\n",
    "\n",
    "\n",
    "datasets = {'train': train_rdkit_mols, 'test': test_rdkit_mols, 'val': val_rdkit_mols}\n",
    "\n",
    "for dataset_name, data in datasets.items():\n",
    "    data_df = pd.DataFrame(data, columns=['structure'])\n",
    "    print(dataset_name)\n",
    "    X = morgan_transformer.transform(data_df.structure)\n",
    "\n",
    "    X_desc = descriptors_transformer.fit_transform(data_df.structure)\n",
    "    descriptors_datasets[dataset_name] = pd.concat([data_df.drop(columns=['structure']), X, X_desc], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols_to_drop = [x for x in descriptors_datasets['train'].columns.to_list() if not isinstance(x, int)]\n",
    "bool_cols_to_drop = [x for x in descriptors_datasets['train'].columns.to_list() if x not in ['TPSA', 'MR']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "separated_features_df = {}\n",
    "for dataset_name, df in descriptors_datasets.items():\n",
    "    num_cols_to_drop = [x for x in descriptors_datasets[f'{dataset_name}'].columns.to_list() if not isinstance(x, int)]\n",
    "    bool_cols_to_drop = [x for x in descriptors_datasets[f'{dataset_name}'].columns.to_list() if x not in ['TPSA', 'MR']]\n",
    "    separated_features_df[f'Xd_{dataset_name}'] = df.drop(columns=bool_cols_to_drop)\n",
    "    separated_features_df[f'Xfp_{dataset_name}'] = df.drop(columns=num_cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(separated_features_df.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xd_train, Xd_test, Xd_val, Xfp_train, Xfp_test, Xfp_val = \\\n",
    "separated_features_df['Xd_train'], separated_features_df['Xd_test'], separated_features_df['Xd_val'], \\\n",
    "separated_features_df['Xfp_train'], separated_features_df['Xfp_test'], separated_features_df['Xfp_val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xd_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### 1.2 Нормализация численных значений\n",
    "\n",
    "**Формула стандартизации (Z-score Normalization)**:\n",
    "\n",
    "$$\n",
    "Y_{\\text{norm}} = \\frac{Y - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "\n",
    "Где:\n",
    "\n",
    "\n",
    "- $Y$ — исходные данные,\n",
    "- $\\mu$ — среднее значение \\( Y \\),\n",
    "- $\\sigma$ — стандартное отклонение \\( Y \\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_scale =  [Xd_train, Xd_test, Xd_val]\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(Xd_train)\n",
    "for n, df in enumerate(df_to_scale):\n",
    "    df = pd.DataFrame(scaler.transform(df), columns=df.columns)\n",
    "    df_to_scale[n] = np.array(df)\n",
    "\n",
    "Xd_train, Xd_test, Xd_val = df_to_scale\n",
    "Xd_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "Приведем датафреймы с отпечатками моргана в нужный формат:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xfp_train, Xfp_test, Xfp_val = np.array(Xfp_train), np.array(Xfp_test), np.array(Xfp_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np.array(train_data['LogP'])\n",
    "# Генерируем данные LogP для тестовой и валидационной выборке, поскольку эти данные требуется предсказать\n",
    "Y_test = np.array([0] * test_data.shape[0])\n",
    "Y_val = np.array([0] * val_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xd_train.shape, Xd_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xfp_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "## 2. Обучение и валидация модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "#### Создание модели на основе нейронной сети в виде многослойного персептрона с одним скрытым слоем"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "Подготовим данные для обучения в нужном формате:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xfp_train = torch.Tensor(Xfp_train)\n",
    "Xfp_test = torch.Tensor(Xfp_test)\n",
    "Xfp_val = torch.Tensor(Xfp_val)\n",
    "Xd_train = torch.Tensor(Xd_train)\n",
    "Xd_test = torch.Tensor(Xd_test)\n",
    "Xd_val = torch.Tensor(Xd_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = torch.Tensor(Y_train).unsqueeze(-1)\n",
    "Y_test = torch.Tensor(Y_test).unsqueeze(-1)\n",
    "Y_val = torch.Tensor(Y_val).unsqueeze(-1)\n",
    "\n",
    "Y_train.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "Определим конфигурацию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 50\n",
    "EPOCHS = 50\n",
    "HIDDEN_SIZE = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "Создадим объект DataLoader - загрузчик данных, который делит входные данные на партии определенного размера - батчи  и подает их на обучение нейронной сети. Создадим загрузчики для каждой из выборок:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogPDataset(Dataset):\n",
    "    def __init__(self, X, y, device):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32).to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dataset = TensorDataset(Xfp_train, Xd_train, Y_train)\n",
    "test_dataset = TensorDataset(Xfp_test, Xd_test, Y_test)\n",
    "val_dataset = TensorDataset(Xfp_val, Xd_val, Y_val)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "Создадим класс LogPModel для определения архитектуры нейронной сети с одним скрытым слоем и двумя входными потоками для обработки отдельно физико-химических дескрипторов и молекулярных отпечатков:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogPModel(pyl.LightningModule):\n",
    "    def __init__(self, fingerprint_size, numeric_features_size, hidden_size=HIDDEN_SIZE):\n",
    "        super(LogPModel, self).__init__()\n",
    "        self.test_predictions = []\n",
    "        self.targets = []\n",
    "\n",
    "        self.fingerprint_fc = nn.Sequential(\n",
    "            nn.Linear(fingerprint_size, hidden_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.numeric_fc = nn.Sequential(\n",
    "            nn.Linear(numeric_features_size, hidden_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "        self.combined_fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        \n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    def forward(self, fingerprint, numeric_features):\n",
    "        fingerprint_out = self.fingerprint_fc(fingerprint)\n",
    "        numeric_out = self.numeric_fc(numeric_features)\n",
    "        \n",
    "        combined = torch.cat([fingerprint_out, numeric_out], dim=1)\n",
    "        \n",
    "        return self.combined_fc(combined)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        fingerprint, numeric_features, y = batch\n",
    "        y_pred = self(fingerprint, numeric_features)\n",
    "\n",
    "        loss = self.loss_fn(y_pred, y)\n",
    "        self.log('Train MSE', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        r2 = r2_score(y.detach().cpu().numpy().reshape(-1), y_pred.detach().cpu().numpy().reshape(-1))\n",
    "        self.log('Train R²', r2, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        fingerprint, numeric_features, y = batch\n",
    "        y_pred = self(fingerprint, numeric_features)\n",
    "\n",
    "        loss = self.loss_fn(y_pred, y)\n",
    "        self.log('Validation MSE', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        r2 = r2_score(y.detach().cpu().numpy().reshape(-1), y_pred.detach().cpu().numpy().reshape(-1))\n",
    "        self.log('Validation R²', r2, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        fingerprint, numeric_features, y = batch\n",
    "        y_pred = self(fingerprint, numeric_features)\n",
    "\n",
    "        loss = self.loss_fn(y_pred, y)\n",
    "        self.log('Test MSE', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        self.test_predictions.extend(y_pred.cpu().numpy())\n",
    "        self.targets.extend(y.cpu().numpy())\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "Создадим и обучим модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogPModel(fingerprint_size=Xfp_train.shape[1], numeric_features_size=Xd_train.shape[1]).to(device)\n",
    "\n",
    "# metrics_callback = MetricsCallback(val_loader, train_loader, device)\n",
    "\n",
    "trainer = pyl.Trainer(max_epochs=EPOCHS, accelerator=\"auto\")\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "Оценим модель на внешней контрольной выборке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model, test_loader)\n",
    "\n",
    "y_pred_test = model.test_predictions\n",
    "y_true = model.targets\n",
    "mse = mean_squared_error(y_true, y_pred_test)\n",
    "q2 = r2_score(y_true, y_pred_test)\n",
    "rmse = np.sqrt(mse)\n",
    "print(f'test MSE: {mse:.4f}, test PRMSE: {rmse:.4f}, test Q²: {q2:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Откроем файл sample_submission и запишем в него предсказание\n",
    "sample_submission = pd.read_csv(\"sample_submission.csv\")\n",
    "# Добавим в качестве предсказаний тестовые данные\n",
    "sample_submission['LogP'] = [round(float(i), 5) for i in y_pred_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраняем прежсказание \n",
    "sample_submission.to_csv(\"prediction.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
