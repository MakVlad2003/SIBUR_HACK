{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchmetrics import MeanSquaredError\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "pl.seed_everything(SEED, workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR   = \"/home/oleg28/shteyn/vlad/SIBUR_HACK/data\"\n",
    "N_FOLDS    = 5\n",
    "BATCH_SIZE = 256\n",
    "MAX_EPOCHS = 300\n",
    "EMB_PCA_DIM = 128          # <=Â 512\n",
    "HIDDEN     = (1024, 768, 512, 256)\n",
    "DROP_P     = 0.40\n",
    "LR_MAX     = 5e-4\n",
    "WD         = 3e-3\n",
    "LABEL_NOISE_STD = 0.05\n",
    "HUBER_DELTA = 1.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emb  = pd.read_pickle(f\"{DATA_DIR}/df_with_embeddings.pkl\")\n",
    "\n",
    "df_desc = (\n",
    "    pd.read_csv(f\"{DATA_DIR}/train_data_all_descriptors.csv\")\n",
    "      .drop(columns=[\"ID\", \"mol\"])\n",
    ")\n",
    "\n",
    "df = pd.merge(df_emb, df_desc, on=[\"SMILES\", \"LogP\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = np.vstack(df[\"embedding\"]).astype(\"float32\")                 # (n, 512)\n",
    "desc = df.drop(columns=[\"SMILES\", \"LogP\", \"embedding\"]).values     # (n, 103)\n",
    "y = df[\"LogP\"].values.astype(\"float32\")\n",
    "\n",
    "\n",
    "full_feat_dim = EMB_PCA_DIM + desc.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(pl.LightningModule):\n",
    "    def __init__(self, in_dim):\n",
    "        super().__init__()\n",
    "        dims, layers = (in_dim, *HIDDEN), []\n",
    "        for i in range(len(dims)-1):\n",
    "            layers += [\n",
    "                nn.Linear(dims[i], dims[i+1]),\n",
    "                nn.LayerNorm(dims[i+1]),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(DROP_P)\n",
    "            ]\n",
    "        layers.append(nn.Linear(dims[-1], 1))\n",
    "        self.net  = nn.Sequential(*layers)\n",
    "        self.huber = nn.SmoothL1Loss(beta=HUBER_DELTA)\n",
    "        self.rmse  = MeanSquaredError(squared=False)\n",
    "\n",
    "    def forward(self, x): return self.net(x).squeeze(1)\n",
    "\n",
    "    def _step(self, batch, tag):\n",
    "        x, y = batch\n",
    "        if tag == \"train\" and LABEL_NOISE_STD > 0:\n",
    "            y = y + torch.randn_like(y)*LABEL_NOISE_STD\n",
    "        y_hat = self(x)\n",
    "        loss  = self.huber(y_hat, y)\n",
    "        rmse  = self.rmse(y_hat, y)\n",
    "        self.log_dict({f\"{tag}_loss\": loss, f\"{tag}_rmse\": rmse}, prog_bar=True)\n",
    "        return loss\n",
    "    def training_step  (self,b,_): return self._step(b,\"train\")\n",
    "    def validation_step(self,b,_):  self._step(b,\"val\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt  = torch.optim.AdamW(self.parameters(), lr=LR_MAX, weight_decay=WD)\n",
    "        sched1 = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            opt, max_lr=LR_MAX, total_steps=self.trainer.estimated_stepping_batches\n",
    "        )\n",
    "        sched2 = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            opt, mode=\"min\", factor=0.5, patience=5, min_lr=1e-5\n",
    "        )\n",
    "        return ([opt],\n",
    "                [{\"scheduler\": sched1, \"interval\": \"step\"},\n",
    "                 {\"scheduler\": sched2, \"interval\": \"epoch\", \"monitor\": \"val_rmse\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(N_FOLDS, shuffle=True, random_state=SEED)\n",
    "oof_pred = np.zeros_like(y)\n",
    "fold_rmse = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold, (tr_idx, vl_idx) in enumerate(kf.split(emb), 1):\n",
    "    emb_tr, emb_vl = emb[tr_idx], emb[vl_idx]\n",
    "    desc_tr, desc_vl = desc[tr_idx], desc[vl_idx]\n",
    "    y_tr, y_vl = y[tr_idx], y[vl_idx]\n",
    "\n",
    "    # ---- PCA fit on train embeddings ----\n",
    "    pca = PCA(n_components=EMB_PCA_DIM, svd_solver=\"full\", random_state=SEED)\n",
    "    emb_tr_pca = pca.fit_transform(emb_tr).astype(\"float32\")\n",
    "    emb_vl_pca = pca.transform(emb_vl).astype(\"float32\")\n",
    "    explained = pca.explained_variance_ratio_.sum()\n",
    "    print(f\"[Fold {fold}]  PCA variance kept: {explained:.3%}\")\n",
    "\n",
    "    X_tr = np.hstack([emb_tr_pca, desc_tr])\n",
    "    X_vl = np.hstack([emb_vl_pca, desc_vl])\n",
    "\n",
    "    scaler = StandardScaler().fit(X_tr)\n",
    "    X_tr = scaler.transform(X_tr).astype(\"float32\")\n",
    "    X_vl = scaler.transform(X_vl).astype(\"float32\")\n",
    "\n",
    "    ds_tr = TensorDataset(torch.from_numpy(X_tr), torch.from_numpy(y_tr))\n",
    "    ds_vl = TensorDataset(torch.from_numpy(X_vl), torch.from_numpy(y_vl))\n",
    "    dl_tr = DataLoader(ds_tr, BATCH_SIZE, True,  num_workers=4, pin_memory=True)\n",
    "    dl_vl = DataLoader(ds_vl, BATCH_SIZE, False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    model = MLP(full_feat_dim)\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=MAX_EPOCHS,\n",
    "        accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "        devices=1,\n",
    "        precision=32,\n",
    "        callbacks=[\n",
    "            EarlyStopping(\"val_rmse\", patience=20, mode=\"min\"),\n",
    "            ModelCheckpoint(save_top_k=1, monitor=\"val_rmse\", mode=\"min\")\n",
    "        ],\n",
    "        log_every_n_steps=25,\n",
    "    )\n",
    "    trainer.fit(model, dl_tr, dl_vl)\n",
    "\n",
    "    best = MLP.load_from_checkpoint(\n",
    "        trainer.checkpoint_callback.best_model_path,\n",
    "        in_dim=full_feat_dim,\n",
    "        map_location=torch.device(\"cpu\")\n",
    "    )\n",
    "    best.eval()\n",
    "    with torch.no_grad():\n",
    "        oof_pred[vl_idx] = best(torch.from_numpy(X_vl)).numpy()\n",
    "\n",
    "    rmse_fold = float(np.sqrt(((oof_pred[vl_idx] - y_vl) ** 2).mean()))\n",
    "    fold_rmse.append(rmse_fold)\n",
    "    print(f\"Fold {fold} RMSE: {rmse_fold:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nOOF RMSE:\", np.sqrt(((oof_pred - y) ** 2).mean()).round(4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moler-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
