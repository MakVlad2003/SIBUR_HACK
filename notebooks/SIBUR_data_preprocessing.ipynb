{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bP6QirfRTil-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# RDKit для фингерпринтов\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "\n",
    "# sklearn для селекции и предобработки\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing     import RobustScaler, StandardScaler\n",
    "from sklearn.decomposition     import PCA\n",
    "from sklearn.ensemble          import RandomForestRegressor\n",
    "from sklearn.inspection        import permutation_importance\n",
    "\n",
    "# molecule-generation для эмбеддингов\n",
    "from molecule_generation import load_model_from_directory\n",
    "\n",
    "# CatBoost и метрики\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Устройство для MoLeR (если используется GPU)\n",
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Using device for embeddings:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LPKOEo4gmNSy"
   },
   "outputs": [],
   "source": [
    "# Пути\n",
    "TRAIN_RAW = \"/home/oleg28/shteyn/vlad/SIBUR_HACK/data/train_data_all_descriptors.csv\"\n",
    "TEST_RAW  = \"/home/oleg28/shteyn/vlad/SIBUR_HACK/data/test_data_all_descriptors.csv\"\n",
    "MODEL_DIR = \"/home/oleg28/shteyn/vlad/SIBUR_HACK/moler/molecule-generation/molecule_generation/model_checkpoint\"\n",
    "OUTPUT    = \"/home/oleg28/shteyn/vlad/SIBUR_HACK/processed\"\n",
    "os.makedirs(OUTPUT, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aWDJ6nG6lxzG"
   },
   "source": [
    "### I. Базовый EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3WczTK_0mGHk"
   },
   "source": [
    "1. Считаем пропуски и дубликаты по SMILES.\n",
    "\n",
    "2. Удаляем полностью дублирующиеся молекулы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(TRAIN_RAW)\n",
    "df_test  = pd.read_csv(TEST_RAW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zifwpSb5mQv-",
    "outputId": "a5be486d-e818-4a18-d2a7-05659debe37b"
   },
   "outputs": [],
   "source": [
    "# Пропуски\n",
    "print(\"Missing in train:\\n\", df_train.isnull().sum().loc[lambda x: x>0])\n",
    "print(\"Missing in test:\\n\",  df_test .isnull().sum().loc[lambda x: x>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1jRyocDjmYa3",
    "outputId": "960aec92-8237-4909-ddfd-6269d1d3e2c4"
   },
   "outputs": [],
   "source": [
    "# Дубликаты SMILES\n",
    "dups = df_train.duplicated(\"SMILES\", keep=False).sum()\n",
    "print(\"SMILES duplicates in train:\", dups)\n",
    "\n",
    "df_train = df_train.drop_duplicates(\"SMILES\").reset_index(drop=True)\n",
    "print(\"Train shape after deduplication:\", df_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18LpUXQ1oIDR"
   },
   "source": [
    "### II. Добавление Morgan‑fingerprints  \n",
    "  \n",
    "`Fingerprint` — это битовая маска, отражающая окружение атомов.  \n",
    "Здесь используем `1024‑битный` `Morgan‑fingerprint` `(radius=2)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smiles_to_fp(smi, radius=2, n_bits=1024):\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    if mol is None:\n",
    "        return np.zeros(n_bits, dtype=int)\n",
    "    fp = rdMolDescriptors.GetMorganFingerprintAsBitVect(mol, radius, n_bits)\n",
    "    arr = np.zeros(n_bits, dtype=int)\n",
    "    arr[list(fp.GetOnBits())] = 1\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерация\n",
    "fps_tr = np.vstack([smiles_to_fp(s) for s in tqdm(df_train[\"SMILES\"], desc=\"Morgan train\")])\n",
    "fps_te = np.vstack([smiles_to_fp(s) for s in tqdm(df_test [\"SMILES\"], desc=\"Morgan test\" )])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrames\n",
    "fp_cols      = [f\"FP_{i}\" for i in range(fps_tr.shape[1])]\n",
    "df_fp_train  = pd.DataFrame(fps_tr, columns=fp_cols)\n",
    "df_fp_test   = pd.DataFrame(fps_te,  columns=fp_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Склейка и сохранение\n",
    "train_morgan = pd.concat([df_train[[\"ID\",\"LogP\"]].reset_index(drop=True), df_fp_train], axis=1)\n",
    "test_morgan  = pd.concat([df_test [[\"ID\"]].reset_index(drop=True), df_fp_test ], axis=1)\n",
    "train_morgan.to_csv(f\"{OUTPUT}/train_morgan.csv\", index=False)\n",
    "test_morgan .to_csv(f\"{OUTPUT}/test_morgan.csv\" , index=False)\n",
    "print(\"Morgan shapes:\", train_morgan.shape, test_morgan.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. MoLeR Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Загружаем модель из:\", MODEL_DIR)\n",
    "with load_model_from_directory(MODEL_DIR, num_workers=8, beam_size=1) as model:\n",
    "    smiles_list = df_train[\"SMILES\"].tolist()\n",
    "    # Генерируем эмбеддинги (возвращается List[np.ndarray])\n",
    "    emb_tr = model.encode(smiles_list)\n",
    "    print(\"Получено эмбеддингов для train:\", len(emb_tr))\n",
    "    smiles_list_te = df_test[\"SMILES\"].tolist()\n",
    "    emb_te = model.encode(smiles_list_te)\n",
    "    print(\"Получено эмбеддингов для test:\", len(emb_te))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Собираем массивы\n",
    "emb_tr = np.vstack(emb_tr)\n",
    "emb_te = np.vstack(emb_te)\n",
    "print(\"Raw embed shapes:\", emb_tr.shape, emb_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA => 50 компонент\n",
    "pca      = PCA(n_components=128, random_state=42)\n",
    "pca_tr   = pca.fit_transform(emb_tr)\n",
    "pca_te   = pca.transform(emb_te)\n",
    "\n",
    "embed_cols     = [f\"embed_pca_{i}\" for i in range(128)]\n",
    "df_embed_train = pd.DataFrame(pca_tr, columns=embed_cols)\n",
    "df_embed_test  = pd.DataFrame(pca_te, columns=embed_cols)\n",
    "df_embed_train[\"ID\"] = df_train[\"ID\"].values\n",
    "df_embed_test [\"ID\"] = df_test [\"ID\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Стандартизация\n",
    "ss = StandardScaler().fit(df_embed_train[embed_cols])\n",
    "df_embed_train[embed_cols] = ss.transform(df_embed_train[embed_cols])\n",
    "df_embed_test [embed_cols]   = ss.transform(df_embed_test [embed_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраняем\n",
    "train_embed = df_embed_train[[\"ID\"]+embed_cols]\n",
    "test_embed  = df_embed_test [[\"ID\"]+embed_cols]\n",
    "train_embed.to_csv(f\"{OUTPUT}/train_embed.csv\", index=False)\n",
    "test_embed .to_csv(f\"{OUTPUT}/test_embed.csv\" , index=False)\n",
    "print(\"Embed shapes:\", train_embed.shape, test_embed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHgQBWeicaer"
   },
   "source": [
    "### IV. Очистка и удаление признаков с нулевой дисперсией\n",
    "  \n",
    "- Убираем колонки `SMILES` и `mol` (RDKit‑объект).  \n",
    "- Сохраняем `ID` и `LogP`.  \n",
    "- Удаляем признаки, где вариация равна нулю."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Объединяем фичи\n",
    "train_full = train_morgan.merge(train_embed, on=\"ID\")\n",
    "test_full  = test_morgan .merge(test_embed,  on=\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6PmI9X40c4Cm"
   },
   "outputs": [],
   "source": [
    "# Zero‑variance filter\n",
    "feat_cols = train_full.columns.drop([\"ID\",\"LogP\"])\n",
    "vt = VarianceThreshold(0.0)\n",
    "vt.fit(train_full[feat_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ge5iR2RSearc",
    "outputId": "1907aded-b9a1-41bc-cb66-4f96227f6e41"
   },
   "outputs": [],
   "source": [
    "keep_cols    = feat_cols[vt.get_support()]\n",
    "removed_zero = feat_cols[~vt.get_support()].tolist()\n",
    "print(\"Removed zero‑var features:\", removed_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H8XO3-J7frp5",
    "outputId": "e03c154e-61c2-4238-ba90-5fbff7d34a73"
   },
   "outputs": [],
   "source": [
    "train_cl = pd.concat([train_full[[\"ID\",\"LogP\"]], train_full[keep_cols]], axis=1)\n",
    "test_cl  = pd.concat([test_full [[\"ID\"]]        , test_full [keep_cols]], axis=1)\n",
    "print(\"After cleaning:\", train_cl.shape, test_cl.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FRaD92h3e7HU"
   },
   "source": [
    "### V. Удаление сильно коррелированных признаков  \n",
    "  \n",
    "Убираем избыточные фичи с |corr| > 0.90."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tbbBBOYNqpLM",
    "outputId": "08e0ad2d-db0f-4254-b748-eb6c76073e0e"
   },
   "outputs": [],
   "source": [
    "corr_mat = train_cl.drop(columns=[\"ID\",\"LogP\"]).corr().abs()\n",
    "upper    = corr_mat.where(np.triu(np.ones(corr_mat.shape), k=1).astype(bool))\n",
    "to_drop  = [c for c in upper.columns if upper[c].gt(0.90).any()]\n",
    "print(\"Dropping correlated:\", len(to_drop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dc = train_cl.drop(columns=to_drop)\n",
    "test_dc  = test_cl .drop(columns=to_drop)\n",
    "print(\"After drop corr:\", train_dc.shape, test_dc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V3pGT-0pq482"
   },
   "source": [
    "### VI. Лог‑преобразование и масштабирование  \n",
    "  \n",
    "- Выявляем фичи со skewness > 1.0.  \n",
    "- Применяем `log1p` (с `clip(lower=0)`).  \n",
    "- Масштабируем через `RobustScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m7JIG4CRq9_e"
   },
   "outputs": [],
   "source": [
    "# Разделяем\n",
    "train_id = train_dc[\"ID\"]\n",
    "y_train  = train_dc[\"LogP\"]\n",
    "X_train  = train_dc.drop(columns=[\"ID\",\"LogP\"])\n",
    "X_test   = test_dc .drop(columns=[\"ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zluY6C47rCbL",
    "outputId": "8085b598-f1af-4047-9492-761190cd5075"
   },
   "outputs": [],
   "source": [
    "# Логируем сильно скошенные >1.0\n",
    "skews  = X_train.skew().abs()\n",
    "to_log = skews[skews > 1.0].index.tolist()\n",
    "print(\"Log1p on:\", to_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in to_log:\n",
    "    X_train[c] = np.log1p(X_train[c].clip(lower=0))\n",
    "    X_test [c] = np.log1p(X_test [c].clip(lower=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SDxn8P-OrGkt"
   },
   "outputs": [],
   "source": [
    "# RobustScaler\n",
    "rs = RobustScaler().fit(X_train)\n",
    "X_train_s = pd.DataFrame(rs.transform(X_train), columns=X_train.columns)\n",
    "X_test_s  = pd.DataFrame(rs.transform(X_test),  columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qCGrvrmVrMcP"
   },
   "outputs": [],
   "source": [
    "train_pre = pd.concat([train_id, X_train_s, y_train], axis=1)\n",
    "test_pre  = pd.concat([test_dc[[\"ID\"]], X_test_s], axis=1)\n",
    "print(\"After log+scale:\", train_pre.shape, test_pre.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7nklk3Dr1t5"
   },
   "source": [
    "### VII. Анализ важности признаков и отбор  \n",
    "  \n",
    "1. Обучим RandomForest на полном наборе.\n",
    "\n",
    "2. Вычислим Permutation Importance (чтобы увидеть не только встроенную «важность» дерева, но и насколько падёт качество при «перемешивании» каждого признака).\n",
    "\n",
    "3. Выберем порог:\n",
    "\n",
    "Уберём фичи с отрицательной или близкой к нулю важностью.\n",
    "\n",
    "Оставим, скажем, топ‑100 или все > 0.001 по mean importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VNn9cNG3t8ju"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mrASbg5NuBk9"
   },
   "outputs": [],
   "source": [
    "X = train_pre.drop(columns=[\"ID\",\"LogP\"])\n",
    "y = train_pre[\"LogP\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 97
    },
    "id": "et8tiRFPuEPL",
    "outputId": "f8be9d54-9e51-47c3-f2f3-58e9e4470be7"
   },
   "outputs": [],
   "source": [
    "# Обучаем RF (быстрее — меньше деревьев, но глубже)\n",
    "rf = RandomForestRegressor(n_estimators=200, max_depth=12, random_state=42, n_jobs=-1)\n",
    "rf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4UgrNDpZuI6m"
   },
   "outputs": [],
   "source": [
    "perm = permutation_importance(rf, X, y, n_repeats=10, random_state=42, n_jobs=-1)\n",
    "imp = pd.DataFrame({\"feature\":X.columns, \"mean_imp\":perm.importances_mean})\n",
    "imp = imp.sort_values(\"mean_imp\", ascending=False).reset_index(drop=True)\n",
    "print(\"Top 20 features:\\n\", imp.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qDK9V7VouOZv"
   },
   "outputs": [],
   "source": [
    "selected = imp.loc[imp.mean_imp>0.001, \"feature\"].tolist()\n",
    "print(f\"Selecting {len(selected)}/{X.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y-mUYFECuRaV"
   },
   "outputs": [],
   "source": [
    "train_final = pd.concat([train_pre[[\"ID\",\"LogP\"]], X[selected]], axis=1)\n",
    "test_final  = pd.concat([test_pre [[\"ID\"]],        test_pre [selected]], axis=1)\n",
    "\n",
    "# Сохраняем финальные таблицы\n",
    "train_final.to_csv(f\"{OUTPUT}/train_final.csv\", index=False)\n",
    "test_final .to_csv(f\"{OUTPUT}/test_final.csv\" , index=False)\n",
    "print(\"Final shapes:\", train_final.shape, test_final.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VII. Обучение CatBoost и оценка RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    train_final.drop(columns=[\"ID\",\"LogP\"]),\n",
    "    train_final[\"LogP\"],\n",
    "    test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostRegressor(\n",
    "    iterations=500,\n",
    "    learning_rate=0.05,\n",
    "    depth=6,\n",
    "    eval_metric=\"RMSE\",\n",
    "    random_seed=42,\n",
    "    verbose=False\n",
    ")\n",
    "model.fit(X_tr, y_tr, eval_set=(X_val, y_val), use_best_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_val)\n",
    "rmse   = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "print(f\"Validation RMSE: {rmse:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "moler-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
